{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba59169",
   "metadata": {},
   "source": [
    "# **Group 14 Apply AI Project: _________________________________**                                   \n",
    "Members: Torrence Barbour, Satwik Nijampudi, Shanthan Gunti, and Ryan Edwards\n",
    "\n",
    "Given a set of posts found on Reddit marked with if the user had depression or not, can we train a model to detect depression in a social media user?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "bd3c8795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to import some libraries to do this, including some big ones like numpy, pandas, and matplotlib\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7546ff38",
   "metadata": {},
   "source": [
    "## Further Cleaning our data                                                                                                  \n",
    "We got our data from a kaggle dataset: https://www.kaggle.com/datasets/infamouscoder/depression-reddit-cleaned and it came pre-cleaned, however we want to further clean this dataset to suit our intended purposes (like lemmatization, removing filler words, and more). We will do this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eaa3243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>is_depression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>we understand that most people who reply immed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>welcome to r depression s check in post a plac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>anyone else instead of sleeping more when depr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i ve kind of stuffed around a lot in my life d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sleep is my greatest and most comforting escap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text  is_depression\n",
       "0  we understand that most people who reply immed...              1\n",
       "1  welcome to r depression s check in post a plac...              1\n",
       "2  anyone else instead of sleeping more when depr...              1\n",
       "3  i ve kind of stuffed around a lot in my life d...              1\n",
       "4  sleep is my greatest and most comforting escap...              1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, we will need to bring in our dataset. We can use numpy for this\n",
    "text = pd.read_csv('depression_dataset_reddit_cleaned.csv')\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7245b628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
      "Requirement already satisfied: setuptools in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (63.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ryan/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ryan/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /home/ryan/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/ryan/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ryan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ryan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ryan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ryan/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ryan/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ryan/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ryan/anaconda3/lib/python3.9/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /home/ryan/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.4.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#Next, we are going to need a tool to tokenize, lemmatize, and manage our data. We will use spacy\n",
    "import spacy\n",
    "!python3 -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b6a56de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_text       anyone else instead of sleeping more when depr...\n",
      "is_depression                                                    1\n",
      "Name: 2, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Just to confirm, lets print out one of the posts\n",
    "print(text.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70f2841",
   "metadata": {},
   "source": [
    "Now that we have some tools, our dataset, and another tool used for NLP installed, we need to start preparing our data so it can be used. The first step in this is something called tokenization. Essentially, each word (or even punctuation for that matter) in a sentence plays a role and it can be looked at as its own entity. We want to break each of these sentences apart into 'tokens' so they are more processable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a62b4847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anyone\n",
      "else\n",
      "instead\n",
      "of\n",
      "sleeping\n",
      "more\n",
      "when\n",
      "depressed\n",
      "stay\n",
      "up\n",
      "all\n",
      "night\n",
      "to\n",
      "avoid\n",
      "the\n",
      "next\n",
      "day\n",
      "from\n",
      "coming\n",
      "sooner\n",
      "may\n",
      "be\n",
      "the\n",
      "social\n",
      "anxiety\n",
      "in\n",
      "me\n",
      "but\n",
      "life\n",
      "is\n",
      "so\n",
      "much\n",
      "more\n",
      "peaceful\n",
      "when\n",
      "everyone\n",
      "else\n",
      "is\n",
      "asleep\n",
      "and\n",
      "not\n",
      "expecting\n",
      "thing\n",
      "of\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "#Lets tokenize the sample sentence from before as an example\n",
    "sample_Tokens = nlp(text.iloc[2]['clean_text'])\n",
    "for part in sample_Tokens: \n",
    "    print(part.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f24bd9",
   "metadata": {},
   "source": [
    "On top of tokenization, there is also lemmatization. A lemma in linguistics is essentially the core component of a word, before conjugation and the like occurs. This is very important because it essentially determines what role the word will play in the sentence. Will it be a noun? A verb? Lets see a few examples of this in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b387bb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lemma of coming is come\n",
      "The lemma of sleeping is sleep\n",
      "The lemma of sooner is soon\n"
     ]
    }
   ],
   "source": [
    "#The .lemma_ command in spacy will output the lemma of the word it was called with. This will come in handy\n",
    "print('The lemma of', sample_Tokens[18], 'is', sample_Tokens[18].lemma_)\n",
    "print('The lemma of', sample_Tokens[4], 'is', sample_Tokens[4].lemma_)\n",
    "print('The lemma of', sample_Tokens[19], 'is', sample_Tokens[19].lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aef281f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The part of speech of the word come is VERB\n",
      "The part of speech of the word soon is ADV\n",
      "The part of speech of the word anxiety is NOUN\n"
     ]
    }
   ],
   "source": [
    "#The .pos_ command will output the part of speech the word plays in a sentence.\n",
    "print('The part of speech of the word', sample_Tokens[18].lemma_, 'is', sample_Tokens[18].pos_)\n",
    "print('The part of speech of the word', sample_Tokens[19].lemma_, 'is', sample_Tokens[19].pos_)\n",
    "print('The part of speech of the word', sample_Tokens[24].lemma_, 'is', sample_Tokens[24].pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf67ab0",
   "metadata": {},
   "source": [
    "Now, lets use this information in order to get our dataset cleaned to the utmost. The only things left should be easily recognizable by future tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d71c919",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (3510832948.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_3542/3510832948.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    to_Clean_More = re.sub(\"[^A-Za-z']+\", ' ', to_Clean['clean_text'].replace('\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#Create an empty array to store the cleaned posts in\n",
    "cleaned_Posts = []\n",
    "\n",
    "#Parse through every post\n",
    "for uncleaned_Post in range(len(text)):\n",
    "    to_Clean = text.iloc[uncleaned_Post].copy() #We need to make a copy or else we will mess with the original data\n",
    "    \n",
    "    #Lets make all the tokens lowercase so capitalization is no longer a concern (We can use PCRE regular expressions)\n",
    "    to_Clean_More = re.sub(\"[^A-Za-z']+\", ' ', to_Clean['clean_text'].lower()\n",
    "    \n",
    "    #Tokenize and Lemmatize the posts now\n",
    "    to_Clean_Even_More = nlp(to_Clean_More)\n",
    "    absurdly_Clean_Now = [word.lemma_ for word in to_Clean_Even_More if ((not word.is_stop) or (' ' in word.text))]\n",
    "    \n",
    "    #If there is any text left in absurdly_Clean_Now the add it to the new column\n",
    "    if len(absurdly_Clean_Now) > 1:\n",
    "        to_Clean['really_Clean'] = ' '.join(absurdly_Clean_Now)\n",
    "    \n",
    "    #Now we just append the row to our cleaned_Posts array\n",
    "    cleaned_Posts.append(to_Clean)\n",
    "    \n",
    "#Create a new data frame with the new column in it\n",
    "clean_Text = pd.DataFrame(cleaned_Posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "303e7c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_text       we understand that most people who reply immed...\n",
      "is_depression                                                    1\n",
      "really_Clean       w e u n d e r s t n d t h t m o s t p e o p ...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#If we save the dataset now we can save a lot of time later, so lets do that\n",
    "clean_Text.to_csv('cleaned_depression_dataset.csv')\n",
    "clean_Text.head()\n",
    "print(clean_Text.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe19bf4-3850-49a6-a45c-1f1f6e1b3f70",
   "metadata": {},
   "source": [
    "This has been a good introduction to how NLP cleans data, but for the sake of efficiency, we are going to use a library known as HuggingFace to get our word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f141b981-78d0-4d89-b47d-659969b5b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as hug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58b69c-96bb-4b67-8739-9a97f8c34a19",
   "metadata": {},
   "source": [
    "With HuggingFace, we can use a model known as BERT which has been trained on tons of the English language in order to tokenize our sentences and turn them into word vectors in just a few commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee3df177-4002-4170-b825-5008e069bdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is adding in the BERT tokenizer which we will use to tokenize the posts\n",
    "theTokenizer = hug.AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f71a21ec-0fd3-400b-b950-ddcad913e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#This is adding in the BERT model which we will use to get the word vectors\n",
    "theModel = hug.AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdb099d-06a2-486c-9c10-3356d3e70789",
   "metadata": {},
   "source": [
    "Now, lets get the word vectors for our posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00e86a2b-e3cd-4145-9938-04fec811ef5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts = pd.read_csv('depression_dataset_reddit_cleaned.csv')\n",
    "words = posts.copy()\n",
    "type(words['clean_text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069be515-865d-4026-bb6c-e2657ae00678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18aaa01293074c708e1baeb3bce15c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7731 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "word_Vectors = []\n",
    "\n",
    "#Parse through every post\n",
    "for x in tqdm(range(len(posts))):\n",
    "    to_Clean = words['clean_text'][x]\n",
    "    inputs = theTokenizer(to_Clean, padding='max_length',truncation = True, max_length = 512, return_tensors=\"pt\")\n",
    "    output = theModel(**inputs)\n",
    "    \n",
    "    word_Vectors.append(output[1])\n",
    "    \n",
    "#Create a new data frame with the new column in it\n",
    "vectors = pd.DataFrame(cleaned_Posts)\n",
    "vectors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4192f22-3186-4bb1-9b6b-ba707f6298f8",
   "metadata": {},
   "source": [
    "And of course, since we do not want to run this again and again, let's save the output to a .csv file in order to keep the data more permanently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb8eb09-e2cc-425c-b42a-98a25b5cf1dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectors' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28028/1665442349.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaned_depression_dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'vectors' is not defined"
     ]
    }
   ],
   "source": [
    "vectors.to_csv('cleaned_depression_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f69c218-a770-4947-a61f-22c9e5554dff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
